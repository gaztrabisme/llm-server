# Mainline llama.cpp — CUDA build for RTX 5080 (sm_120)
# Multi-stage: build in devel image, run in runtime image

# === Build Stage ===
FROM nvidia/cuda:12.8.1-devel-ubuntu24.04 AS builder

RUN apt-get update && apt-get install -y --no-install-recommends \
    git cmake build-essential pkg-config \
    && rm -rf /var/lib/apt/lists/*

ARG LLAMA_CPP_REF=""

WORKDIR /build

# Clone llama.cpp — optionally pin to a specific tag/commit via LLAMA_CPP_REF
RUN if [ -n "$LLAMA_CPP_REF" ]; then \
      git clone --depth 1 --branch "$LLAMA_CPP_REF" https://github.com/ggml-org/llama.cpp.git . ; \
    else \
      git clone --depth 1 https://github.com/ggml-org/llama.cpp.git . ; \
    fi

# Make CUDA driver stub findable by the linker (real libcuda.so is injected at runtime by nvidia-container-toolkit)
# Need both libcuda.so and libcuda.so.1 — the shared lib records DT_NEEDED for libcuda.so.1
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/lib/x86_64-linux-gnu/libcuda.so.1 \
    && ldconfig

# Build with CUDA, sm_120 (Blackwell), flash attention all quants
RUN cmake -B build \
    -DGGML_CUDA=ON \
    -DCMAKE_CUDA_ARCHITECTURES=120 \
    -DGGML_CUDA_FA_ALL_QUANTS=ON \
    -DCMAKE_BUILD_TYPE=Release \
    && cmake --build build --config Release -j$(nproc) \
    && cmake --install build --prefix /opt/llama-cpp

# === Runtime Stage ===
FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04

RUN apt-get update && apt-get install -y --no-install-recommends \
    curl libgomp1 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /opt/llama-cpp /opt/llama-cpp

# Add to PATH and library search path
ENV PATH="/opt/llama-cpp/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/llama-cpp/lib:${LD_LIBRARY_PATH}"

# Default model mount point
VOLUME /models

EXPOSE 8080

HEALTHCHECK --interval=10s --timeout=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

ENTRYPOINT ["llama-server"]
