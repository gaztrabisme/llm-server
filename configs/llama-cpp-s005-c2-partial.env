# Session 005 C2: Qwen3.5-35B-A3B Q8_0 â€” Partial MoE offload
# Key optimization: keep some MoE layers on GPU, reduce PCIe transfers
# --n-cpu-moe N: N layers offloaded to CPU (40 - N stay on GPU)
# Sweep values: 8, 16, 24, 32 via bench.sh cpu_moe_count override
LLAMA_ARG_MODEL=/models/Qwen3.5-35B-A3B-Q8_0.gguf
LLAMA_ARG_CTX_SIZE=65536
LLAMA_ARG_N_GPU_LAYERS=999
LLAMA_ARG_FLASH_ATTN=true
LLAMA_ARG_THREADS=20
LLAMA_ARG_BATCH_SIZE=4096
LLAMA_ARG_UBATCH_SIZE=4096
LLAMA_ARG_NO_MMAP=true
LLAMA_ARG_JINJA=true
LLAMA_ARG_HOST=0.0.0.0
LLAMA_ARG_PORT=8080
LLAMA_ARG_CACHE_TYPE_K=q8_0
LLAMA_ARG_CACHE_TYPE_V=q8_0
LLAMA_ARG_N_CPU_MOE=16
