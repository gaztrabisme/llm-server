# ik_llama.cpp â€” Optimized configuration (B2)
# Baseline + ik-specific MoE optimizations + thread sweep
LLAMA_ARG_MODEL=/models/Qwen3-Next-80B-A3B-Instruct-Q8_0.gguf
LLAMA_ARG_CTX_SIZE=32768
LLAMA_ARG_N_GPU_LAYERS=999
LLAMA_ARG_OVERRIDE_TENSOR=exps=CPU
LLAMA_ARG_FLASH_ATTN=true
LLAMA_ARG_THREADS=16
# NOTE: -b 4096 -ub 4096 causes SIGSEGV in ik_llama.cpp with this model
# Using defaults (2048/512) which work correctly
#LLAMA_ARG_BATCH_SIZE=4096
#LLAMA_ARG_UBATCH_SIZE=4096
LLAMA_ARG_NO_MMAP=true
LLAMA_ARG_JINJA=true
LLAMA_ARG_HOST=0.0.0.0
LLAMA_ARG_PORT=8080
# ik_llama.cpp specific optimizations:
# --merge-qkv (-mqkv): fuse Q/K/V into single matrix multiply
# -ger: grouped expert routing (different from -gr which is graph reuse)
# fused_moe is enabled by default in ik_llama.cpp
IK_EXTRA_ARGS=--merge-qkv -ger
# Thread count will be swept via bench.sh (8, 16, 24)
