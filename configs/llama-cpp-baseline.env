# Mainline llama.cpp â€” Baseline configuration (A1)
# Standard MoE offloading: all layers on GPU, experts on CPU
LLAMA_ARG_MODEL=/models/Qwen3-Next-80B-A3B-Instruct-Q8_0.gguf
LLAMA_ARG_CTX_SIZE=32768
LLAMA_ARG_N_GPU_LAYERS=999
LLAMA_ARG_OVERRIDE_TENSOR=exps=CPU
LLAMA_ARG_FLASH_ATTN=true
LLAMA_ARG_THREADS=16
LLAMA_ARG_BATCH_SIZE=4096
LLAMA_ARG_UBATCH_SIZE=4096
LLAMA_ARG_NO_MMAP=true
LLAMA_ARG_JINJA=true
LLAMA_ARG_HOST=0.0.0.0
LLAMA_ARG_PORT=8080
